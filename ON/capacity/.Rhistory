# Credit_COVID19_Canada
The primary data source is the TransUnion credit bureau &reg;.
Data are provided to the Bank of Canada on a monthly basis.
Under the contractual agreement with TransUnion,
the data are not publicly available.
The Bank of Canada does, however, have a process for external researchers
to work with these data.
The Bank of Canada's Financial System Research Center
is a hub for research on household finance
(https://www.bankofcanada.ca/research/financial-system-research-centre/).
Interested parties, who are Canadian citizens or permanent residents,
can contact Jason Allen (Jallen@bankofcanada.ca)
or the Managing Director of research Jim MacGee (JMacGee@bankofcanada.ca).
Interested parties are asked to submit a project proposal;
the proposal is evaluated by senior staff at the Bank of Canada for feasibility;
external researchers do not typically have direct access to the data
and must work with a Bank of Canada staff member.
An exception is if an external collaborator applies for
and is granted temporary employee status -- in this case,
the external researcher can access the data
so long as they have a Bank of Canada affiliation.
All research is vetted by Bank of Canada senior staff prior to publication.
source("C:/Users/atyho/Dropbox/tu_LiM/Empirical-Application/estimation/2018/BC/graph.R")
rm(list = ls())
source("C:/Users/atyho/Desktop/IFC_DS_CB/program/R_graph.R")
dist_data <- read.csv(file = 'dist_data.csv', stringsAsFactors = TRUE)
install.packages(c("askpass", "backports", "BH", "bibtex", "bit", "blogdown", "bookdown", "brew", "brio", "broom", "bslib", "cachem", "Cairo", "callr", "car", "carData", "CDM", "chron", "cli", "clipr", "colorspace", "commonmark", "conquer", "copula", "corrplot", "cpp11", "crayon", "credentials", "cubature", "curl", "data.table", "desc", "DescTools", "devtools", "diffobj", "digest", "DistributionUtils", "doBy", "doParallel", "dplyr", "e1071", "evaluate", "Exact", "expm", "fansi", "farver", "fastmap", "FNN", "forcats", "foreach", "fs", "generics", "gert", "ggplot2", "ggpubr", "ggrepel", "ggsci", "ggsignif", "gh", "gitcreds", "gld", "glue", "gsl", "gtable", "haven", "highr", "hms", "htmltools", "httpuv", "httr", "isoband", "iterators", "jsonlite", "kdevine", "kernlab", "knitr", "ks", "labeling", "later", "lifecycle", "lme4", "lmom", "locfit", "LowRankQP", "lubridate", "magrittr", "maptools", "markdown", "MatrixModels", "matrixStats", "mclust", "memoise", "microbenchmark", "mime", "minqa", "mnormt", "mvtnorm", "nloptr", "np", "openssl", "openxlsx", "optimx", "pbapply", "pbkrtest", "pcaPP", "pillar", "pkgbuild", "pkgload", "plyr", "polycor", "polynom", "pracma", "processx", "promises", "proxy", "ps", "pspline", "psych", "purrr", "qrng", "quantreg", "randtoolbox", "rcmdcheck", "RColorBrewer", "Rcpp", "RcppArmadillo", "RcppEigen", "readr", "readxl", "RefManageR", "rematch", "remotes", "rio", "rlang", "rmarkdown", "rngWELL", "rootSolve", "roxygen2", "rprojroot", "rstatix", "rstudioapi", "rugarch", "rversions", "sass", "scales", "servr", "sessioninfo", "shiny", "sourcetools", "sp", "statar", "stringi", "stringr", "Synth", "sys", "TAM", "testthat", "tibble", "tidyr", "tidyselect", "tinytex", "truncnorm", "tzdb", "usethis", "utf8", "vctrs", "VineCopula", "viridisLite", "vroom", "waldo", "whisker", "withr", "xfun", "xml2", "xts", "yaml", "zip", "zoo"))
install.packages("VineCopula")
install.packages("QRMlib")
install.packages("copulaedas")
install.packages("copula")
install.packages("Rtools")
install.packages(c("admisc", "arrow", "askpass", "cachem", "Cairo", "chron", "cubature", "curl", "DescTools", "digest", "DistributionUtils", "dplyr", "fs", "gert", "haven", "htmltools", "httpuv", "jsonlite", "ks", "later", "lme4", "lmom", "locfit", "LowRankQP", "maptools", "Matrix", "MatrixModels", "matrixStats", "microbenchmark", "minqa", "mvtnorm", "openssl", "processx", "profvis", "promises", "purrr", "quantreg", "Rcpp", "RcppArmadillo", "readxl", "rlang", "sass", "sp", "sys", "testthat", "tzdb", "vctrs", "VineCopula", "vroom", "xfun", "xml2"))
install.packages(c("admisc", "arrow", "askpass", "cachem", "Cairo", "chron", "cubature", "curl", "DescTools", "digest", "DistributionUtils", "dplyr", "fs", "gert", "haven", "htmltools", "httpuv", "jsonlite", "ks", "later", "lme4", "lmom", "locfit", "LowRankQP", "maptools", "Matrix", "MatrixModels", "matrixStats", "microbenchmark", "minqa", "mvtnorm", "openssl", "processx", "profvis", "promises", "purrr", "quantreg", "Rcpp", "RcppArmadillo", "readxl", "rlang", "sass", "sp", "sys", "testthat", "tzdb", "vctrs", "VineCopula", "vroom", "xfun", "xml2"))
install.packages("VineCopula")
list()
getwd()
previously_installed <- read.csv('previously_installed.csv')
package_list <- as.character(previously_installed$Package)
package_list
install.lib <- package_list[!package_list %in% installed.packages()]
for(lib in install.lib) install.packages(lib, dependencies = TRUE)
for(lib in install.lib) install.packages(lib, dependencies = TRUE)
getwd()
previously_installed <- read.csv('previously_installed.csv')
package_list <- as.character(previously_installed$Package)
package_list
install.lib <- package_list[!package_list %in% installed.packages()]
for(lib in install.lib) install.packages(lib, dependencies = TRUE)
install.packages("VGAM")
getwd()
setwd
setwd("C:\Users\atyho\OneDrive\Desktop\eletricity_data\ON")
rm(ls())
rm(list())
library(readr)
library(readr)
#import and merge all three CSV files into one data frame
df <- list.files(path='../demand')
getwd()
library(dplyr)
getwd()
setwd()
setwd('C:\Users\atyho\OneDrive\Desktop\eletricity_data\ON')
setwd("'"C:/Users/atyho/OneDrive/Desktop/eletricity_data/ON")
setwd("C:/Users/atyho/OneDrive/Desktop/eletricity_data/ON")
getwd
getwd()
#import and merge all three CSV files into one data frame
df <- list.files(path='../demand')
list()
getwd()
# Install the XML package if not already installed
install.packages("XML")
# Load the XML package
library(XML)
# Parse the XML file
xml_file <- xmlTreeParse("PUB_GenOutputbyFuelHourly_2023.xml", useInternalNodes = TRUE)
# Get the root node of the XML
root_node <- xmlRoot(xml_file)
# Print the root node to see the structure
print(root_node)
# Install xml2 package if not already installed
install.packages("xml2")
# Load the xml2 package
library(xml2)
# Read the XML file
xml_file <- read_xml("PUB_GenOutputbyFuelHourly_2023.xml")
# Extract the daily data
daily_data <- xml_find_all(xml_file, ".//DailyData")
# Initialize an empty list to store the results
results <- list()
# Loop through each daily entry
for (day_node in daily_data) {
# Extract the date
day <- xml_text(xml_find_first(day_node, ".//Day"))
# Extract the hourly data for each day
hourly_data <- xml_find_all(day_node, ".//HourlyData")
# Loop through each hour
for (hour_node in hourly_data) {
# Extract the hour
hour <- xml_text(xml_find_first(hour_node, ".//Hour"))
# Extract the fuel types and their outputs
fuels <- xml_find_all(hour_node, ".//FuelTotal")
# Loop through each fuel type
for (fuel_node in fuels) {
# Extract fuel type and output
fuel <- xml_text(xml_find_first(fuel_node, ".//Fuel"))
output <- xml_text(xml_find_first(fuel_node, ".//Output"))
# Append to the results list
results <- append(results, list(data.frame(
Day = day,
Hour = hour,
Fuel = fuel,
Output = as.numeric(output),
stringsAsFactors = FALSE
)))
}
}
}
# Combine the list into a single data frame
final_df <- do.call(rbind, results)
# Display the final data frame
print(final_df)
source("C:/Users/atyho/Dropbox/eletricity_data/ON/test.R")
install.packages("xml2")
# Load the xml2 package
library(xml2)
source("C:/Users/atyho/Dropbox/eletricity_data/ON/test.R")
source("C:/Users/atyho/Dropbox/eletricity_data/ON/test.R")
View(final_df)
# Step 1: Combine Day and Hour into a Date column in Eastern Standard Time (EST)
final_df <- final_df %>%
mutate(
Date = as.POSIXct(paste(Day, Hour, ":00:00", sep=" "), format="%Y-%m-%d %H:%M:%S", tz="UTC") %>%
with_tz("America/New_York")
)
# Step 2: Reshape the data to wide format
wide_df <- final_df %>%
select(Date, Fuel, Output) %>%   # Select only relevant columns
pivot_wider(names_from = Fuel, values_from = Output)
# Load the required packages
library(xml2)
library(tidyr)
library(dplyr)
library(lubridate)
# Step 1: Combine Day and Hour into a Date column in Eastern Standard Time (EST)
final_df <- final_df %>%
mutate(
Date = as.POSIXct(paste(Day, Hour, ":00:00", sep=" "), format="%Y-%m-%d %H:%M:%S", tz="UTC") %>%
with_tz("America/New_York")
)
# Step 2: Reshape the data to wide format
wide_df <- final_df %>%
select(Date, Fuel, Output) %>%   # Select only relevant columns
pivot_wider(names_from = Fuel, values_from = Output)
# Display the wide data frame
print(wide_df)
View(wide_df)
rm(ls = list())
rm(list = ls())
rm(list = ls())
# Load the required packages
library(xml2)
library(tidyr)
library(dplyr)
library(lubridate)
# Read the XML file
xml_file <- read_xml("PUB_GenOutputbyFuelHourly_2023.xml")
# Define the namespace (replace 'd' with any prefix, e.g., 'd')
ns <- c(d = "http://www.ieso.ca/schema")
# Extract the daily data with namespace
daily_data <- xml_find_all(xml_file, ".//d:DailyData", ns)
# Initialize an empty list to store the results
results <- list()
# Loop through each daily entry
for (day_node in daily_data) {
# Extract the date
day <- xml_text(xml_find_first(day_node, ".//d:Day", ns))
# Extract the hourly data for each day
hourly_data <- xml_find_all(day_node, ".//d:HourlyData", ns)
# Loop through each hour
for (hour_node in hourly_data) {
# Extract the hour
hour <- xml_text(xml_find_first(hour_node, ".//d:Hour", ns))
# Extract the fuel types and their outputs
fuels <- xml_find_all(hour_node, ".//d:FuelTotal", ns)
# Loop through each fuel type
for (fuel_node in fuels) {
# Extract fuel type and output
fuel <- xml_text(xml_find_first(fuel_node, ".//d:Fuel", ns))
output <- xml_text(xml_find_first(fuel_node, ".//d:Output", ns))
# Append to the results list
results <- append(results, list(data.frame(
Day = day,
Hour = hour,
Fuel = fuel,
Output = as.numeric(output),
stringsAsFactors = FALSE
)))
}
}
}
# Combine the list into a single data frame
final_df <- do.call(rbind, results)
final_df <- final_df %>% arrange(Fuel, Date)
# Step 1: Combine Day and Hour into a Date column in Eastern Standard Time (EST)
final_df <- final_df %>%
mutate(
Date = as.POSIXct(paste(Day, Hour, ":00:00", sep=" "), format="%Y-%m-%d %H:%M:%S", tz="UTC") %>%
with_tz("America/New_York")
)
View(final_df)
# Step 1: Combine Day and Hour into a Date column in Eastern Standard Time (EST)
final_df <- final_df %>%
mutate(
Date = as.POSIXct(paste(Day, Hour, ":00:00", sep=" "), format="%Y-%m-%d %H:%M:%S", tz="UTC") %>%
with_tz("America/New_York")
)
# Step 1: Combine Day and Hour into a Date column in Eastern Standard Time (EST)
final_df <- final_df %>%
mutate(
Date = as.POSIXct(paste(Day, Hour, ":00:00", sep=" "), format="%Y-%m-%d %H:%M:%S", tz="UTC")
)
View(final_df)
# Step 1: Combine Day and Hour into a Date column in Eastern Standard Time (EST)
final_df <- final_df %>%
mutate(
Date = as.POSIXct(paste(Day, sprintf("%02d:00:00", Hour)), format="%Y-%m-%d %H:%M:%S", tz="UTC") %>%
with_tz("America/New_York")
)
# Step 1: Combine Day and Hour into a Date column in Eastern Standard Time (EST)
final_df <- final_df %>%
mutate(
Date = as.POSIXct(paste(Day, sprintf("%02d:00:00", Hour)), format="%Y-%m-%d %H:%M:%S", tz="UTC") %>%
with_tz("America/New_York")
)
# Step 1: Combine Day and Hour into a Date column in Eastern Standard Time (EST)
final_df <- final_df %>%
mutate(
# Combine Day and Hour, formatting Hour correctly with leading zeros if necessary
Date = as.POSIXct(paste(Day, sprintf("%02d:00:00", as.integer(Hour))), format="%Y-%m-%d %H:%M:%S", tz="UTC") %>%
with_tz("America/New_York")
)
# Step 1: Combine Day and Hour into a Date column in Eastern Standard Time (EST)
final_df <- final_df %>%
mutate(
# Combine Day and Hour, formatting Hour correctly with leading zeros if necessary
Date = as.POSIXct(paste(Day, sprintf("%02d:00:00", as.integer(Hour))),
format="%Y-%m-%d %H:%M:%S", tz="America/New_York")
)
View(final_df)
final_df <- final_df %>% arrange(Fuel, Date)
# Step 1: Combine Day and Hour into a Date column in Eastern Standard Time (EST)
final_df <- final_df %>%
mutate(
# Combine Day and Hour, formatting Hour correctly with leading zeros if necessary
Date = as.POSIXct(paste(Day, sprintf("%02d:00:00", as.integer(Hour))),
format="%Y-%m-%d %H:%M:%S", tz="Etc/GMT+5")
)
View(final_df)
final_df <- final_df %>% arrange(Fuel, Date)
# Step 2: Reshape the data to wide format
wide_df <- final_df %>%
select(Date, Fuel, Output) %>%   # Select only relevant columns
pivot_wider(names_from = Fuel, values_from = Output)
View(wide_df)
# Reshape the data to wide format with Day and Hour
wide_df <- final_df %>%
pivot_wider(names_from = Fuel, values_from = Output, values_fill = 0) %>% # Fill missing values with 0
mutate(Total_Output = rowSums(select(., NUCLEAR, GAS, HYDRO, WIND, SOLAR, BIOFUEL))) %>% # Calculate Total Output
select(Day, Hour, NUCLEAR, GAS, HYDRO, WIND, SOLAR, BIOFUEL, Total_Output) # Reorder columns
View(wide_df)
# Reshape the data to wide format with Day and Hour
wide_df <- final_df %>%
pivot_wider(names_from = Fuel, values_from = Output, values_fill = 0) %>% # Fill missing values with 0
mutate(Total_Output = rowSums(select(., NUCLEAR, GAS, HYDRO, WIND, SOLAR, BIOFUEL))) %>% # Calculate Total Output
select(Day, Hour, NUCLEAR, GAS, HYDRO, WIND, SOLAR, BIOFUEL, Total_Output) %>% # Reorder columns
rename(Date = Day) # Rename the column 'Day' to 'Date'
source("C:/Users/atyho/Dropbox/eletricity_data/ON/test.R")
rm(list = ls())
library(dplyr)
library(readr)
library(stringr)
library(here)
library(lubridate)
######################
# Electricity Demand #
######################
setwd(paste0(here(), "/demand"))
# Import and merge all CSV files into one data frame, skipping the first 3 lines
load_ON <- list.files() %>%
lapply(function(file) read_csv(file, skip = 3, show_col_types = FALSE)) %>%
bind_rows()
# Rename columns
names(load_ON) <- c("Date","Hour","demand_mkt","demand_ON")
# Format date column
# Eastern Standard Time is used year round
load_ON <- load_ON %>% mutate(Date = ymd(Date)) %>%
mutate(time = ymd_h(paste(Date, Hour), tz = "Etc/GMT+5")) %>%
mutate(time_utc = with_tz(time, tzone = "UTC")) %>%
select(-Date, -Hour, -time)
setwd(paste0(here(), "/fuel_mix"))
# Import and merge all CSV files into one data frame
fuelmix_ON <- list.files() %>%
lapply(read_csv, show_col_types = FALSE) %>%
bind_rows
# Format date column
# Eastern Standard Time is used year round
fuelmix_ON <- fuelmix_ON %>% mutate(Date = mdy(Date)) %>%
mutate(time = ymd_h(paste(Date, Hour), tz = "Etc/GMT+5")) %>%
mutate(time_utc = with_tz(time, tzone = "UTC")) %>%
select(-Date, -Hour, -time)
IESO <- inner_join(load_ON, fuelmix_ON, by = "time_utc")
View(fuelmix_ON)
source("C:/Users/atyho/Dropbox/eletricity_data/ON/xml_read.R")
rm(list = ls())
library(dplyr)
library(readr)
library(stringr)
library(here)
library(lubridate)
######################
# Electricity Demand #
######################
setwd(paste0(here(), "/demand"))
# Import and merge all CSV files into one data frame, skipping the first 3 lines
load_ON <- list.files() %>%
lapply(function(file) read_csv(file, skip = 3, show_col_types = FALSE)) %>%
bind_rows()
# Rename columns
names(load_ON) <- c("Date","Hour","demand_mkt","demand_ON")
# Format date column
# Eastern Standard Time is used year round
load_ON <- load_ON %>% mutate(Date = ymd(Date)) %>%
mutate(time = ymd_h(paste(Date, Hour), tz = "Etc/GMT+5")) %>%
mutate(time_utc = with_tz(time, tzone = "UTC")) %>%
select(-Date, -Hour, -time)
######################
# Electricity Supply #
######################
setwd(paste0(here(), "/fuel_mix"))
# Import and merge all CSV files into one data frame
fuelmix_ON <- list.files() %>%
lapply(read_csv, show_col_types = FALSE) %>%
bind_rows
# Format date column
# Eastern Standard Time is used year round
fuelmix_ON <- fuelmix_ON %>% mutate(Date = mdy(Date)) %>%
mutate(time = ymd_h(paste(Date, Hour), tz = "Etc/GMT+5")) %>%
mutate(time_utc = with_tz(time, tzone = "UTC")) %>%
select(-Date, -Hour, -time)
IESO <- inner_join(load_ON, fuelmix_ON, by = "time_utc")
View(fuelmix_ON)
# Save the dataframe as an RData file
save(load_ON, file = "../load_IESO.RData")
save(fuelmix_ON, file = "../fuelmix_IESO.RData")
save(IESO, file = "../IESO.RData")
setwd(paste0(here(),"/capacity"))
# Import and merge all CSV files into one data frame
capacity_ON <- list.files() %>%
lapply(function(file) read_csv(file, skip = 3, col_select = 1:28, show_col_types = FALSE)) %>%
bind_rows
# Import and merge all CSV files into one data frame
# Read and preprocess the CSV files to remove trailing commas
capacity_ON <- list.files(pattern = "*.csv") %>%
lapply(function(file) {
# Read lines and remove trailing commas
lines <- read_lines(file)
cleaned_lines <- str_remove(lines, ",+$")
# Parse cleaned lines into a data frame
read_csv(paste(cleaned_lines, collapse = "\n"), skip = 3, col_select = 1:28, show_col_types = FALSE)
}) %>%
bind_rows()
View(capacity_ON)
names(capacity_ON) <- c("Date", "Generator", "Fuel_Type",	"Measurement",
"Hour1", "Hour2", "Hour3", "Hour4", "Hour5", "Hour6",
"Hour7", "Hour8",	"Hour9", "Hour10", "Hour11", "Hour12",
"Hour13", "Hour14", "Hour15", "Hour16", "Hour17", "Hour18",
"Hour19", "Hour20", "Hour21", "Hour22",	"Hour23",	"Hour24")
capacity_ON <- capacity_ON %>%
filter(Fuel_Type == "HYDRO") %>%
arrange(Generator, Measurement, Date) %>%
mutate(time = ymd_h(paste(date, hour), tz = "Etc/GMT+5")) %>%
mutate(time_utc = with_tz(time, tzone = "UTC")) %>%
arrange(time) %>% select(-date, -hour)
View(capacity_ON)
View(capacity_ON)
# Filter for fuel type
capacity_ON <- capacity_ON %>%
filter(Fuel_Type == "HYDRO")
# Step 2: Reshape to long format for Hour
capacity_long <- capacity_ON %>%
pivot_longer(cols = starts_with("Hour"), names_to = "Hour", values_to = "Value") %>%
mutate(Hour = parse_number(Hour)) # Convert 'Hour1', 'Hour2',... to 1, 2,...
View(capacity_long)
# Step 3: Summarize the data by Date, Hour, and Measurement
capacity_summarized <- capacity_long %>%
group_by(Date, Hour, Measurement) %>%
summarise(Total_Value = sum(Value, na.rm = TRUE), .groups = 'drop')
# Step 2: Reshape to long format for Hour
capacity_long <- capacity_ON %>%
pivot_longer(cols = starts_with("Hour"), names_to = "Hour", values_to = "Value") %>%
mutate(Hour = parse_number(Hour)) # Convert 'Hour1', 'Hour2',... to 1, 2,...
# Step 3: Summarize the data by Date, Hour, and Measurement
capacity_summarized <- capacity_long %>%
group_by(Date, Hour, Measurement) %>%
summarise(Total_Value = sum(Value, na.rm = TRUE), .groups = 'drop')
View(capacity_summarized)
# Step 4: Pivot to wide format with each Measurement as a column
capacity_wide <- capacity_summarized %>%
pivot_wider(names_from = Measurement, values_from = Total_Value)
View(capacity_wide)
# Step 4: Pivot to wide format with each Measurement as a column
capacity_wide <- capacity_summarized %>%
pivot_wider(names_from = Measurement, values_from = Total_Value) %>%
mutate(time = ymd_h(paste(date, hour), tz = "Etc/GMT+5")) %>%
mutate(time_utc = with_tz(time, tzone = "UTC")) %>%
arrange(time) %>% select(-Date, -Hour, -time)
# Step 4: Pivot to wide format with each Measurement as a column
capacity_wide <- capacity_summarized %>%
pivot_wider(names_from = Measurement, values_from = Total_Value) %>%
mutate(time = ymd_h(paste(Date, Hour), tz = "Etc/GMT+5")) %>%
mutate(time_utc = with_tz(time, tzone = "UTC")) %>%
arrange(time) %>% select(-Date, -Hour, -time)
View(capacity_wide)
setwd(paste0(here(),"/capacity"))
# Read and preprocess the CSV files to remove trailing commas
capacity_ON <- list.files(pattern = "*.csv") %>%
lapply(function(file) {
# Read lines and remove trailing commas
lines <- read_lines(file)
cleaned_lines <- str_remove(lines, ",+$")
# Parse cleaned lines into a data frame
read_csv(paste(cleaned_lines, collapse = "\n"), skip = 3, col_select = 1:28, show_col_types = FALSE)
}) %>%
bind_rows()
names(capacity_ON) <- c("Date", "Generator", "Fuel_Type",	"Measurement",
"Hour1", "Hour2", "Hour3", "Hour4", "Hour5", "Hour6",
"Hour7", "Hour8",	"Hour9", "Hour10", "Hour11", "Hour12",
"Hour13", "Hour14", "Hour15", "Hour16", "Hour17", "Hour18",
"Hour19", "Hour20", "Hour21", "Hour22",	"Hour23",	"Hour24")
View(capacity_ON)
# Filter for fuel type
capacity_ON <- capacity_ON %>% filter(Fuel_Type == "HYDRO")
# Reshape to long format for Hour
capacity_ON <- capacity_ON %>%
pivot_longer(cols = starts_with("Hour"),
names_to = "Hour", values_to = "Value") %>%
mutate(Hour = parse_number(Hour))
# Summarize the data by Date, Hour, and Measurement
capacity_ON <- capacity_ON %>%
group_by(Fuel_Type, Date, Hour, Measurement) %>%
summarise(Total_Value = sum(Value, na.rm = TRUE), .groups = 'drop')
# Pivot to wide format with each Measurement as a column
capacity_ON <- capacity_ON %>%
pivot_wider(names_from = Measurement, values_from = Total_Value) %>%
mutate(time = ymd_h(paste(Date, Hour), tz = "Etc/GMT+5")) %>%
mutate(time_utc = with_tz(time, tzone = "UTC")) %>%
arrange(time) %>% select(-Date, -Hour, -time)
setwd(paste0(here(), "/fuel_mix"))
# Import and merge all CSV files into one data frame
fuelmix_ON <- list.files() %>%
lapply(read_csv, show_col_types = FALSE) %>%
bind_rows
# Format date column
# Eastern Standard Time is used year round
fuelmix_ON <- fuelmix_ON %>% mutate(Date = mdy(Date)) %>%
mutate(time = ymd_h(paste(Date, Hour), tz = "Etc/GMT+5")) %>%
mutate(time_utc = with_tz(time, tzone = "UTC")) %>%
select(-Date, -Hour)
View(fuelmix_ON)
source("C:/Users/atyho/Dropbox/eletricity_data/ON/ON_data_process.R")
source("C:/Users/atyho/Dropbox/eletricity_data/ON/ON_data_process.R")
source("C:/Users/atyho/Dropbox/eletricity_data/ON/ON_data_process.R")
#### Hydro Capacity ####
setwd(paste0(here(),"/capacity"))
source("C:/Users/atyho/Dropbox/eletricity_data/ON/ON_data_process.R")
#### Hydro Capacity ####
setwd(paste0(here(), "/capacity"))
IESO <- inner_join(load_ON, fuelmix_ON, by = c("time","time_utc")
setwd(paste0(here(), "/capacity"))
IESO <- inner_join(load_ON, fuelmix_ON, by = c("time","time_utc"))
source("C:/Users/atyho/Dropbox/eletricity_data/ON/ON_data_process.R")
